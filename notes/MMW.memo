先前講到使用現有語料建立辭彙庫，有別於使用傳統字典的方式，可以即時反應特定領域的辭彙概念。有了辭彙庫（不論是從字典來的，或是從統計得來）
，接下來，就可以處理中文斷詞的問題。


中文斷詞（Chinese Word Segmentation）是中文自然語言處理最基本、最經典的議題。中文和大多數的西方語文不同，詞（word）與詞之間，沒有空白作分隔。由於詞彙是表達語文概念的基本單位，從連續的漢字字元中決定出一個一個詞彙，乃為相當重要的基本處理步驟。


目前效能較好的中文斷詞模型，多半採用機器學器的方式，而這些方式都須要大量的訓練資料。所謂訓練資料，就是預先標記好正確答案的例句，電腦的學習模型會從這些例句裡粹取出判斷的知識。然而，訓練資料多半需要語言背景的專家，為大量的資料一一標記答案（以中文斷詞為例，專家要為許許多多中文例句斷好詞），才能獲得。目前固然有的現成訓練資料可以取得，但這些資料行之有年，又多半是泛用領域（general domain）的資料。當我們要處理新領域的文章，文句裡充滿了新的、特定領域的詞彙時，舊有的訓練資料不見得適用。


另一個問題是，機器學習的模型效能雖佳，然而多半複雜。不論是實作的複雜度，或是運算效能都會成為實務運用上的障礙。例如要在極短的反應時間下斷好詞，甚至要在client端用js執行，都不太可能使用效能最佳的那幾種模型。因此，這時一些不需要訓練而且簡易的演算法，雖然理論上效能較差，但卻有實務上的優點。


在此介紹一個相當簡單的中文斷詞演算法，只要有一套詞典，不需要訓練，實作也相當容易，可以當作基礎模型來使用。方法如下：


給定一個連續的中文字串S，以及詞典D。詞典D裡最長的詞，長度為w個漢字。
令S(i, j)表示S的第i個漢字至第j個漢字所構成的子字串。
i從0開始，從左到右掃描S。對每個i，先取S(i, i+w)這段子字串出來，檢查S(i, i+w)是否在D中。有則將S(i, i+w)視為一個詞輸出，然後i跳至i+w+1，
繼續掃描。若S(i, i+w)不在D中，則檢查S(i, i+w-1)是否在D中，再檢查S(i, i+w-2)、S(i, i+w-3)等等直至S(i, i+1)為止。如S(i, i+1)仍不在D中，
則將S(i,i)輸出，視為一個單字詞。最後傳回適才依序輸出的每一個詞，就是斷好詞的結果。


這是典型的「貪心演算法」（Greedy Algorithm）.和人類的閱讀順序一樣，由左至右掃描輸入字串，依序檢查當前的詞彙是否在字典中。如果同時符合多個詞彙，則以長詞優先。


舉例來說，輸入的文句是「剛剛才發佈的智慧型手機」，

而字典裡的詞有「剛剛、剛才、發佈、智慧、手機、智慧型手機」，


從左開始掃描，依序會找到「剛剛」、「才」、「發佈」、「的」、「智慧型手機」，

而「剛才」、「智慧」、「手機」這三個詞則不會被發掘。


長詞優先的好處是可以儘量偵測出一些長的、特別的詞彙，而這些特別的詞彙，又往往包含了重要資訊。以上例來說，「智慧型手機」這個詞會比斷成「智慧」、「型」、「手機」這三個詞更具意義。雖然以語文處理來說的角度，這個方法還不夠精確，容易有錯誤。但從實務觀點來看，這個方法夠快、夠簡單，還能找到重要資訊，是很實用的方法


